{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Data Modeling with Cassandra\n",
    "\n",
    "A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analysis team is particularly interested in understanding what songs users are listening to. Currently, there is no easy way to query the data to generate the results, since the data reside in a directory of CSV files on user activity on the app.\n",
    "\n",
    "They'd like a data engineer to create an Apache Cassandra database which can create queries on song play data to answer the questions, and wish to bring you on the project. Your role is to create a database for this analysis. You'll be able to test your database by running queries given to you by the analytics team from Sparkify to create the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: The ETL pipeline\n",
    "\n",
    "In this part we'll read a bunch of raw CSV files, parse them and store the result in a new aggregate CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules\n",
    "\n",
    "We must import all the modules on which we rely at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement, BatchType, ConsistencyLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "It's important to avoid hardcoded magic values along the code. It's always a good practice to keep them in constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current working directory.\n",
    "HERE = os.getcwd()\n",
    "\n",
    "# The directory where the raw CSV files are stored.\n",
    "INPUT_DIR = os.path.join(HERE, \"event_data\")\n",
    "\n",
    "# The pattern to use for recursive raw CSV file search.\n",
    "INPUT_DIR_SEARCH_PATTERN = os.path.join(INPUT_DIR, \"**\", \"*.csv\")\n",
    "\n",
    "# The path to the aggregate CSV.\n",
    "OUTPUT_CSV = os.path.join(HERE, \"event_datafile_new.csv\")\n",
    "\n",
    "# The size of the chunks in which the CSV files will be loaded.\n",
    "CHUNK_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate CSV creation\n",
    "\n",
    "We aggregate multiple raw CSV files into one that can be used as master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = 0\n",
    "written = 0\n",
    "\n",
    "# Prepare the aggregate CSV for writing.\n",
    "with open(OUTPUT_CSV, \"w\", encoding=\"utf8\", newline=\"\") as output_file:\n",
    "\n",
    "    writer = csv.writer(\n",
    "        output_file,\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        skipinitialspace=True\n",
    "    )\n",
    "\n",
    "    # Write a first row that represents the header.\n",
    "    writer.writerow((\n",
    "        \"artist_name\",\n",
    "        \"user_first_name\",\n",
    "        \"item_in_session\",\n",
    "        \"user_last_name\",\n",
    "        \"song_length\",\n",
    "        \"session_id\",\n",
    "        \"song_title\",\n",
    "        \"user_id\"\n",
    "    ))\n",
    "\n",
    "    # Walk down through the directory where the raw CSV files are stored.\n",
    "    for path in glob.glob(INPUT_DIR_SEARCH_PATTERN, recursive=True):\n",
    "\n",
    "        # Prepare one by one for reading.\n",
    "        with open(path, \"r\", encoding=\"utf8\", newline=\"\") as input_file:\n",
    "\n",
    "            reader = csv.reader(input_file)\n",
    "\n",
    "            # Every raw CSV file has a header that we must skip.\n",
    "            next(reader)\n",
    "\n",
    "            # Once skipped, we iterate over the remaining rows.\n",
    "            for row in reader:\n",
    "\n",
    "                read += 1\n",
    "\n",
    "                # Check if the current event is music related.\n",
    "                if row[10] == \"NextSong\":\n",
    "\n",
    "                    # If it is, we write it down to the aggregate CSV file (only the columns we need).\n",
    "                    writer.writerow((\n",
    "                        row[0],\n",
    "                        row[2],\n",
    "                        row[4],\n",
    "                        row[5],\n",
    "                        row[6],\n",
    "                        row[12],\n",
    "                        row[13],\n",
    "                        row[16]\n",
    "                    ))\n",
    "\n",
    "                    written += 1\n",
    "\n",
    "print(\"Read: {}\\nWritten: {}\\nDiscarded: {}\".format(read, written, read - written))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Working with Cassandra\n",
    "\n",
    "In this part we'll read the aggregate CSV file an push the rows to the Cassandra's cluster. Later we'll query it to check that the job was successfully accomplished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the database\n",
    "\n",
    "At this point, we initialize the database. It consist in the following three steps:\n",
    "\n",
    "1. Connect with the Cassandra's cluster\n",
    "2. Create the keyspace if it doesn't exist yet\n",
    "3. Set the created keyspace in the current cluster session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Connect with the cluster.\n",
    "    cluster = Cluster(['127.0.0.1'])\n",
    "    session = cluster.connect()\n",
    "\n",
    "    # Create the keyspace.\n",
    "    session.execute(\"\"\"\n",
    "        CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "        WITH REPLICATION = {\n",
    "            'class': 'SimpleStrategy',\n",
    "            'replication_factor': 1\n",
    "        }\n",
    "    \"\"\")\n",
    "\n",
    "    # Set the keyspace.\n",
    "    session.set_keyspace(\"sparkify\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error while initializing the database\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs by session\n",
    "\n",
    "In order to execute queries like this:\n",
    "\n",
    "> Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4\n",
    "\n",
    "We create the table `songs_by_session`. We set its primary key using the fields we later use to filter the records: `session_id` and `item_in_session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS songs_by_session (\n",
    "            session_id int,\n",
    "            item_in_session int,\n",
    "            artist_name text,\n",
    "            song_title text,\n",
    "            song_length float,\n",
    "            PRIMARY KEY (\n",
    "                session_id,\n",
    "                item_in_session\n",
    "            )\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Songs by user\n",
    "\n",
    "Given the following query as an example:\n",
    "\n",
    "> Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "We create the table `songs_by_user`. We use the fields `user_id`, `session_id` and `item_in_session` as its primary key: the first two fields will be used in the `WHERE` clause, while the third one will be used for sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS songs_by_user (\n",
    "            user_id int,\n",
    "            session_id int,\n",
    "            item_in_session int,\n",
    "            artist_name text,\n",
    "            song_title text,\n",
    "            user_first_name text,\n",
    "            user_last_name text,\n",
    "            PRIMARY KEY (\n",
    "                user_id,\n",
    "                session_id,\n",
    "                item_in_session\n",
    "            )\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users by song\n",
    "\n",
    "And last but not least, the table `users_by_song`.\n",
    "\n",
    "> Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "This is the simplest table of the given three. In fact, it has no composite primary key like the previous tables; instead, we use `song_title` as primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS users_by_song (\n",
    "            song_title text,\n",
    "            user_first_name text,\n",
    "            user_last_name text,\n",
    "            PRIMARY KEY (\n",
    "                song_title\n",
    "            )\n",
    "        )\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch processing\n",
    "\n",
    "Because the size of our data can be huge, we must take care of the performance while working with databases. One thing we can do is to bulk insert the events in the cluster, instead of insert them one by one. Cassandra support batch operations, so we are going to take advantage from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table songs_by_session\n",
    "\n",
    "Here is the insert query. Also, a list with the columns we will use from the aggregate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the insert query.\n",
    "songs_by_session_insert_query = session.prepare(\"\"\"\n",
    "    INSERT INTO songs_by_session (\n",
    "        session_id,\n",
    "        item_in_session,\n",
    "        artist_name,\n",
    "        song_title,\n",
    "        song_length\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# The columns from the aggregate CSV file we will use in the query.\n",
    "songs_by_session_columns = [\n",
    "    \"session_id\",\n",
    "    \"item_in_session\",\n",
    "    \"artist_name\",\n",
    "    \"song_title\",\n",
    "    \"song_length\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table songs_by_user\n",
    "\n",
    "Here is the insert query. Also, a list with the columns we will use from the aggregate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the insert query.\n",
    "songs_by_user_insert_query = session.prepare(\"\"\"\n",
    "    INSERT INTO songs_by_user (\n",
    "        user_id,\n",
    "        session_id,\n",
    "        item_in_session,\n",
    "        artist_name,\n",
    "        song_title,\n",
    "        user_first_name,\n",
    "        user_last_name\n",
    "    )\n",
    "    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# The columns from the aggregate CSV file we will use in the query.\n",
    "songs_by_user_columns = [\n",
    "    \"user_id\",\n",
    "    \"session_id\",\n",
    "    \"item_in_session\",\n",
    "    \"artist_name\",\n",
    "    \"song_title\",\n",
    "    \"user_first_name\",\n",
    "    \"user_last_name\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table users_by_song\n",
    "\n",
    "Here is the insert query. Also, a list with the columns we will use from the aggregate CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the insert query.\n",
    "users_by_song_insert_query = session.prepare(\"\"\"\n",
    "    INSERT INTO users_by_song (\n",
    "        song_title,\n",
    "        user_first_name,\n",
    "        user_last_name\n",
    "    )\n",
    "    VALUES (?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# The columns from the aggregate CSV file we will use in the query.\n",
    "users_by_song_columns = [\n",
    "    \"song_title\",\n",
    "    \"user_first_name\",\n",
    "    \"user_last_name\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the process\n",
    "\n",
    "We're ready to take the aggregate CSV file into Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batches = 0\n",
    "records = 0\n",
    "\n",
    "# Create a batch statement to handle bulk inserts.\n",
    "batch = BatchStatement(\n",
    "    batch_type=BatchType.UNLOGGED,\n",
    "    consistency_level=ConsistencyLevel.ALL\n",
    ")\n",
    "\n",
    "# Process the aggregate CSV file in chunks.\n",
    "for df in pd.read_csv(OUTPUT_CSV, chunksize=CHUNK_SIZE):\n",
    "\n",
    "    # For every event in the current chunk...\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        # ...we add the table songs_by_session's insert query to the batch\n",
    "        batch.add(\n",
    "            songs_by_session_insert_query,\n",
    "            row[songs_by_session_columns].values.tolist()\n",
    "        )\n",
    "\n",
    "        # ...also the one for the table songs_by_user\n",
    "        batch.add(\n",
    "            songs_by_user_insert_query,\n",
    "            row[songs_by_user_columns].values.tolist()\n",
    "        )\n",
    "\n",
    "        # ...and lastly the one for the table users_by_song\n",
    "        batch.add(\n",
    "            users_by_song_insert_query,\n",
    "            row[users_by_song_columns].values.tolist()\n",
    "        )\n",
    "        \n",
    "        records += 3\n",
    "\n",
    "    try:\n",
    "        # Execute all the queries in the batch.\n",
    "        session.execute(batch)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while processing a batch statement\")\n",
    "        print(e)\n",
    "\n",
    "    else:\n",
    "        # Cassandra's batch processing is limited; we must empty it to reuse.\n",
    "        batch.clear()\n",
    "        batches += 1\n",
    "\n",
    "print(\"Batches: {}\\nRecords: {}\".format(batches, records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The moment of truth\n",
    "\n",
    "It's time to query Cassandra to check if the aggregate CSV file was successfully imported in the defined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1\n",
    "\n",
    "This query tests the following request:\n",
    "\n",
    "> Give me the artist, song title and song's length in the music app history that was heard during sessionId = 338, and itemInSession = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT artist_name,\n",
    "               song_title,\n",
    "               song_length\n",
    "          FROM songs_by_session\n",
    "         WHERE session_id = %s\n",
    "           AND item_in_session = %s\n",
    "    \"\"\", (338, 4))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(\"{} | {} | {}\".format(\n",
    "        row.artist_name,\n",
    "        row.song_title,\n",
    "        row.song_length\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2\n",
    "\n",
    "This query tests the following request:\n",
    "\n",
    "> Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT artist_name,\n",
    "               song_title,\n",
    "               user_first_name,\n",
    "               user_last_name\n",
    "          FROM songs_by_user\n",
    "         WHERE user_id = %s\n",
    "           AND session_id = %s\n",
    "    \"\"\", (10, 182))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(\"{} | {} | {} {}\".format(\n",
    "        row.artist_name,\n",
    "        row.song_title,\n",
    "        row.user_first_name,\n",
    "        row.user_last_name\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3\n",
    "\n",
    "This query tests the following request:\n",
    "\n",
    "> Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rows = session.execute(\"\"\"\n",
    "        SELECT user_first_name,\n",
    "               user_last_name\n",
    "          FROM users_by_song\n",
    "         WHERE song_title = %s\n",
    "    \"\"\", (\"All Hands Against His Own\",))\n",
    "except Exception as e:\n",
    "    print(\"Error while initializing the database\")\n",
    "    print(e)\n",
    "    \n",
    "for row in rows:\n",
    "    print(\"{} {}\".format(\n",
    "        row.user_first_name,\n",
    "        row.user_last_name\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables\n",
    "\n",
    "Once the ETL process is tested, we can drop the tables used for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session.execute(\"DROP TABLE songs_by_session\")\n",
    "    session.execute(\"DROP TABLE songs_by_user\")\n",
    "    session.execute(\"DROP TABLE users_by_song\")\n",
    "except Exception as e:\n",
    "    print(\"Error while dropping the table\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session\n",
    "\n",
    "And finally, we free the connection to the Cassandra's cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
